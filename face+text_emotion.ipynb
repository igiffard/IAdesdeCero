{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Curso IA desde Cero**\n",
        "\n",
        "* Dr. Irvin Hussein L√≥pez Nava\n",
        "* M.C. Joan M. Raygoza Romero"
      ],
      "metadata": {
        "id": "jaPBvMoom1HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalar las librerias necesarias para imagen, keypoints y transcripci√≥n de audio a texto"
      ],
      "metadata": {
        "id": "3XhafOB6lNOr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZaKwJFuhJFi"
      },
      "outputs": [],
      "source": [
        "!pip install mediapipe opencv-python moviepy SpeechRecognition pydub\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraer keypoints y features para el modelo de visi√≥n"
      ],
      "metadata": {
        "id": "zh8okRY0nSgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import pickle\n",
        "\n",
        "# =========================\n",
        "# 1. Definici√≥n de keypoints\n",
        "# =========================\n",
        "\n",
        "IMPORTANT_KEYPOINTS = {\n",
        "    # ---- REFERENCIAS / NORMALIZACI√ìN ----\n",
        "    \"nose_tip\": 1,\n",
        "    \"nose_bridge\": 6,\n",
        "    \"forehead_center\": 10,\n",
        "    \"chin\": 152,\n",
        "\n",
        "    # ---- BOCA (EXTERNA E INTERNA) ----\n",
        "    \"mouth_left\": 61,\n",
        "    \"mouth_right\": 291,\n",
        "    \"mouth_upper_outer\": 13,\n",
        "    \"mouth_lower_outer\": 14,\n",
        "    \"mouth_upper_inner\": 0,\n",
        "    \"mouth_lower_inner\": 17,\n",
        "    \"mouth_left_inner\": 40,\n",
        "    \"mouth_right_inner\": 270,\n",
        "    \"mouth_corner_left_top\": 84,\n",
        "    \"mouth_corner_left_bottom\": 181,\n",
        "    \"mouth_corner_right_top\": 314,\n",
        "    \"mouth_corner_right_bottom\": 405,\n",
        "\n",
        "    # ---- OJO IZQUIERDO ----\n",
        "    \"left_eye_outer\": 33,\n",
        "    \"left_eye_inner\": 133,\n",
        "    \"left_eye_upper\": 159,\n",
        "    \"left_eye_lower\": 145,\n",
        "    \"left_eye_upper_inner\": 158,\n",
        "    \"left_eye_lower_inner\": 153,\n",
        "    \"left_eye_upper_outer\": 160,\n",
        "    \"left_eye_lower_outer\": 144,\n",
        "\n",
        "    # ---- OJO DERECHO ----\n",
        "    \"right_eye_outer\": 263,\n",
        "    \"right_eye_inner\": 362,\n",
        "    \"right_eye_upper\": 386,\n",
        "    \"right_eye_lower\": 374,\n",
        "    \"right_eye_upper_inner\": 385,\n",
        "    \"right_eye_lower_inner\": 380,\n",
        "    \"right_eye_upper_outer\": 387,\n",
        "    \"right_eye_lower_outer\": 373,\n",
        "\n",
        "    # ---- CEJA IZQUIERDA ----\n",
        "    \"left_eyebrow_outer\": 70,\n",
        "    \"left_eyebrow_middle\": 105,\n",
        "    \"left_eyebrow_inner\": 107,\n",
        "    \"left_eyebrow_lower_outer\": 46,\n",
        "    \"left_eyebrow_lower_middle\": 52,\n",
        "    \"left_eyebrow_lower_inner\": 55,\n",
        "\n",
        "    # ---- CEJA DERECHA ----\n",
        "    \"right_eyebrow_outer\": 300,\n",
        "    \"right_eyebrow_middle\": 334,\n",
        "    \"right_eyebrow_inner\": 336,\n",
        "    \"right_eyebrow_lower_outer\": 285,\n",
        "    \"right_eyebrow_lower_middle\": 282,\n",
        "    \"right_eyebrow_lower_inner\": 276\n",
        "}\n",
        "\n",
        "IMPORTANT_KEYPOINTS_IDX_LIST = list(IMPORTANT_KEYPOINTS.values())\n",
        "KEY_NAMES = list(IMPORTANT_KEYPOINTS.keys())\n",
        "\n",
        "# √≠ndices de los ojos para normalizaci√≥n\n",
        "LEFT_EYE_INNER_IDX = IMPORTANT_KEYPOINTS[\"left_eye_inner\"]\n",
        "RIGHT_EYE_INNER_IDX = IMPORTANT_KEYPOINTS[\"right_eye_inner\"]\n",
        "\n",
        "# preparar face mesh\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "\n",
        "# =========================\n",
        "# 2. Funciones auxiliares\n",
        "# =========================\n",
        "\n",
        "def extract_keypoints_from_frame(img_bgr, face_mesh,\n",
        "                                normalization=True,\n",
        "                                head_orientation=True):\n",
        "    \"\"\"\n",
        "    img_bgr: frame en BGR (OpenCV)\n",
        "    face_mesh: instancia de mp.solutions.face_mesh.FaceMesh\n",
        "    Devuelve: vector 1D con coords [x1,y1,x2,y2,...] normalizadas,\n",
        "              o None si no hay rostro.\n",
        "    \"\"\"\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(img_rgb)\n",
        "\n",
        "    if not results.multi_face_landmarks:\n",
        "        return None\n",
        "\n",
        "    face_landmarks = results.multi_face_landmarks[0]\n",
        "\n",
        "    # Extraer solo keypoints importantes\n",
        "    important_landmarks = []\n",
        "    for idx in IMPORTANT_KEYPOINTS_IDX_LIST:\n",
        "        lm = face_landmarks.landmark[idx]\n",
        "        important_landmarks.append([lm.x, lm.y])\n",
        "\n",
        "    keypoints_array = np.array(important_landmarks)  # (N, 2)\n",
        "\n",
        "    # Ojos para referencia (usamos inner)\n",
        "    le_lm = face_landmarks.landmark[LEFT_EYE_INNER_IDX]\n",
        "    re_lm = face_landmarks.landmark[RIGHT_EYE_INNER_IDX]\n",
        "    left_eye  = np.array([le_lm.x, le_lm.y])\n",
        "    right_eye = np.array([re_lm.x, re_lm.y])\n",
        "\n",
        "    # 1) Centrar en el medio de los ojos\n",
        "    anchor = (left_eye + right_eye) / 2\n",
        "    keypoints_array = keypoints_array - anchor\n",
        "\n",
        "    # 2) Escala (distancia entre ojos)\n",
        "    if normalization:\n",
        "        eye_vec  = right_eye - left_eye\n",
        "        eye_dist = np.linalg.norm(eye_vec)\n",
        "        if eye_dist < 1e-6:\n",
        "            eye_dist = 1e-6\n",
        "        keypoints_array = keypoints_array / eye_dist\n",
        "\n",
        "    # 3) Rotaci√≥n (alinear ojos al eje X)\n",
        "    if head_orientation:\n",
        "        eye_vec = right_eye - left_eye\n",
        "        angle = np.arctan2(eye_vec[1], eye_vec[0])\n",
        "        R = np.array([\n",
        "            [np.cos(-angle), -np.sin(-angle)],\n",
        "            [np.sin(-angle),  np.cos(-angle)]\n",
        "        ])\n",
        "        keypoints_array = keypoints_array @ R.T\n",
        "\n",
        "    return keypoints_array.flatten()"
      ],
      "metadata": {
        "id": "sfgVW9KiiU5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distance(a, b):\n",
        "    \"\"\"Distancia euclidiana 2D simple.\"\"\"\n",
        "    return float(np.linalg.norm(a - b))\n",
        "\n",
        "\n",
        "def build_feature_vector(keypoints):\n",
        "    pts = keypoints.reshape(-1, 2)  # (n_points, 2)\n",
        "\n",
        "    # Mapeo nombre -> punto (x,y)\n",
        "    P = {name: pts[i] for i, name in enumerate(KEY_NAMES)}\n",
        "\n",
        "    feats = []\n",
        "\n",
        "    # ---------- BOCA ----------\n",
        "    mouth_width = distance(P[\"mouth_left\"], P[\"mouth_right\"])\n",
        "    mouth_open_outer = abs(P[\"mouth_upper_outer\"][1] - P[\"mouth_lower_outer\"][1])\n",
        "    mouth_open_inner = abs(P[\"mouth_upper_inner\"][1] - P[\"mouth_lower_inner\"][1])\n",
        "\n",
        "    # ‚Äúcurvatura‚Äù de la sonrisa (boca hacia arriba/abajo)\n",
        "    mean_corner_y = 0.5 * (P[\"mouth_left\"][1] + P[\"mouth_right\"][1])\n",
        "    smile_curvature = mean_corner_y - P[\"mouth_lower_outer\"][1]\n",
        "\n",
        "    feats += [\n",
        "        mouth_width,\n",
        "        mouth_open_outer,\n",
        "        mouth_open_inner,\n",
        "        smile_curvature,\n",
        "    ]\n",
        "\n",
        "    # ---------- OJOS ----------\n",
        "    # ojo izquierdo\n",
        "    left_eye_open = distance(P[\"left_eye_upper\"], P[\"left_eye_lower\"])\n",
        "    left_eye_open_inner = distance(P[\"left_eye_upper_inner\"], P[\"left_eye_lower_inner\"])\n",
        "    left_eye_open_outer = distance(P[\"left_eye_upper_outer\"], P[\"left_eye_lower_outer\"])\n",
        "\n",
        "    # ojo derecho\n",
        "    right_eye_open = distance(P[\"right_eye_upper\"], P[\"right_eye_lower\"])\n",
        "    right_eye_open_inner = distance(P[\"right_eye_upper_inner\"], P[\"right_eye_lower_inner\"])\n",
        "    right_eye_open_outer = distance(P[\"right_eye_upper_outer\"], P[\"right_eye_lower_outer\"])\n",
        "\n",
        "    feats += [\n",
        "        left_eye_open,\n",
        "        left_eye_open_inner,\n",
        "        left_eye_open_outer,\n",
        "        right_eye_open,\n",
        "        right_eye_open_inner,\n",
        "        right_eye_open_outer,\n",
        "    ]\n",
        "\n",
        "    # ---------- CEJAS (altura respecto al ojo y pendiente) ----------\n",
        "    left_eye_center = 0.5 * (P[\"left_eye_inner\"] + P[\"left_eye_outer\"])\n",
        "    right_eye_center = 0.5 * (P[\"right_eye_inner\"] + P[\"right_eye_outer\"])\n",
        "\n",
        "    left_brow_height = left_eye_center[1] - P[\"left_eyebrow_middle\"][1]\n",
        "    right_brow_height = right_eye_center[1] - P[\"right_eyebrow_middle\"][1]\n",
        "\n",
        "    # pendiente (inclinaci√≥n) de ceja: outer vs inner\n",
        "    left_brow_slope = P[\"left_eyebrow_outer\"][1] - P[\"left_eyebrow_inner\"][1]\n",
        "    right_brow_slope = P[\"right_eyebrow_outer\"][1] - P[\"right_eyebrow_inner\"][1]\n",
        "\n",
        "    feats += [\n",
        "        left_brow_height,\n",
        "        right_brow_height,\n",
        "        left_brow_slope,\n",
        "        right_brow_slope,\n",
        "    ]\n",
        "\n",
        "    # ---------- PROPORCIONES CARA ----------\n",
        "    # altura de cara: frente -> ment√≥n\n",
        "    face_height = distance(P[\"forehead_center\"], P[\"chin\"])\n",
        "    # longitud nariz: nose_bridge -> nose_tip\n",
        "    nose_length = distance(P[\"nose_bridge\"], P[\"nose_tip\"])\n",
        "\n",
        "    # algunos ratios que son invariantes de escala\n",
        "    mouth_width_ratio = mouth_width / face_height\n",
        "    nose_face_ratio = nose_length / face_height\n",
        "\n",
        "    feats += [\n",
        "        face_height,\n",
        "        nose_length,\n",
        "        mouth_width_ratio,\n",
        "        nose_face_ratio,\n",
        "    ]\n",
        "\n",
        "    return np.array(feats)"
      ],
      "metadata": {
        "id": "uIYxvDROii_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predecir una emocion con los keypoints del video"
      ],
      "metadata": {
        "id": "Uh54vrwknkrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_emotion_from_video(clip, model, fps_sample=5):\n",
        "    \"\"\"\n",
        "    Toma frames del video a una cierta frecuencia (fps_sample),\n",
        "    obtiene keypoints y promedia las probabilidades.\n",
        "    \"\"\"\n",
        "    probs_list = []\n",
        "    with mp_face_mesh.FaceMesh(\n",
        "        static_image_mode=False,\n",
        "        max_num_faces=1,\n",
        "        refine_landmarks=True,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    ) as face_mesh:\n",
        "\n",
        "        times = np.arange(0, clip.duration, 1.0 / fps_sample)\n",
        "        for t in times:\n",
        "            frame_rgb = clip.get_frame(t)  # RGB (moviepy da RGB)\n",
        "            kp_flat = extract_keypoints_from_frame(frame_rgb, face_mesh)\n",
        "            if kp_flat is None:\n",
        "                continue\n",
        "            X = kp_flat.reshape(1, -1)\n",
        "            X = build_feature_vector(X).reshape(1, -1)\n",
        "            proba = img_model.predict_proba(X)[0]\n",
        "            probs_list.append(proba)\n",
        "\n",
        "    if not probs_list:\n",
        "        return None  # no se pudo detectar rostro en ning√∫n frame\n",
        "\n",
        "    probs_mean = np.mean(probs_list, axis=0)\n",
        "    return probs_mean"
      ],
      "metadata": {
        "id": "2eDeuKeSi8RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparar la transcripci√≥n del audio y extracci√≥n de embeddings del texto"
      ],
      "metadata": {
        "id": "yk2oZvuvoE3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import speech_recognition as sr\n",
        "\n",
        "emb_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "def transcribe_audio(audio_path, language=\"es-MX\"):\n",
        "    r = sr.Recognizer()\n",
        "    with sr.AudioFile(audio_path) as source:\n",
        "        audio = r.record(source)\n",
        "    try:\n",
        "        text = r.recognize_google(audio, language=language)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(\"Error al transcribir:\", e)\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def predict_emotion_from_text(text, model, emb_model):\n",
        "    if not text.strip():\n",
        "        return None\n",
        "    embs = emb_model.encode(text)\n",
        "    proba = model.predict_proba([embs])[0]\n",
        "    return proba"
      ],
      "metadata": {
        "id": "01MsPDPNnvEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar modelos"
      ],
      "metadata": {
        "id": "HGKqfbpLnyYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_MODEL_PATH = \"model.pkl\"\n",
        "IMG_LABEL_ENCODER_PATH = \"le.pkl\"\n",
        "TEXT_MODEL_PATH = \"model_text.pkl\"\n",
        "TEXT_LABEL_ENCODER_PATH = \"le_text.pkl\"\n",
        "\n",
        "with open(IMG_MODEL_PATH, \"rb\") as f:\n",
        "    img_model = pickle.load(f)\n",
        "\n",
        "with open(IMG_LABEL_ENCODER_PATH, \"rb\") as f:\n",
        "    img_le = pickle.load(f)\n",
        "\n",
        "with open(TEXT_MODEL_PATH, \"rb\") as f:\n",
        "    text_model = pickle.load(f)\n",
        "\n",
        "with open(TEXT_LABEL_ENCODER_PATH, \"rb\") as f:\n",
        "    text_le = pickle.load(f)"
      ],
      "metadata": {
        "id": "i9Y3Nt8Ciuew"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funcion para fusionar las probabilidades del video y texto"
      ],
      "metadata": {
        "id": "G_875UU0oZSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fuse_probs(p_video, p_text, alpha=0.5):\n",
        "    \"\"\"\n",
        "    alpha: peso para el video (0.0‚Äì1.0)\n",
        "    \"\"\"\n",
        "    if p_video is None and p_text is None:\n",
        "        return None\n",
        "    if p_video is None:\n",
        "        return p_text\n",
        "    if p_text is None:\n",
        "        return p_video\n",
        "    return alpha * p_video + (1 - alpha) * p_text"
      ],
      "metadata": {
        "id": "hu-GtdPljqxz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Capturar un video y obtener el audio"
      ],
      "metadata": {
        "id": "3ZaoLT-_3ZeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "import numpy as np\n",
        "import base64\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "VIDEO_PATH = \"captura.webm\"\n",
        "\n",
        "# Funci√≥n robusta para grabar video con audio en Colab\n",
        "def grabar_video(segundos=10):\n",
        "  js = f\"\"\"\n",
        "    async function recordVideo() {{\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({{video: true, audio: true}});\n",
        "      const options = {{mimeType: \"video/webm;codecs=vp9,opus\"}};\n",
        "\n",
        "      const mediaRecorder = new MediaRecorder(stream, options);\n",
        "      let chunks = [];\n",
        "\n",
        "      mediaRecorder.ondataavailable = (e) => {{\n",
        "        if (e.data.size > 0) chunks.push(e.data);\n",
        "      }};\n",
        "\n",
        "      mediaRecorder.start();\n",
        "\n",
        "      // bot√≥n para detener\n",
        "      const btn = document.createElement(\"button\");\n",
        "      btn.textContent = \"‚èπÔ∏è DETENER\";\n",
        "      btn.style = \"font-size:20px; margin:10px;\";\n",
        "      document.body.appendChild(btn);\n",
        "\n",
        "      let stopped = false;\n",
        "      btn.onclick = () => {{\n",
        "        if (!stopped) {{\n",
        "          mediaRecorder.stop();\n",
        "          stopped = true;\n",
        "        }}\n",
        "      }};\n",
        "\n",
        "      // tiempo m√°ximo\n",
        "      await new Promise(resolve => setTimeout(resolve, {segundos * 1000}));\n",
        "\n",
        "      if (!stopped) mediaRecorder.stop();\n",
        "\n",
        "      await new Promise(resolve => mediaRecorder.onstop = resolve);\n",
        "\n",
        "      document.body.removeChild(btn);\n",
        "      stream.getTracks().forEach(t => t.stop());\n",
        "\n",
        "      const blob = new Blob(chunks, {{type: \"video/webm\"}});\n",
        "      const reader = new FileReader();\n",
        "\n",
        "      return await new Promise(resolve => {{\n",
        "        reader.onloadend = () => resolve(reader.result);\n",
        "        reader.readAsDataURL(blob);\n",
        "      }});\n",
        "    }}\n",
        "\n",
        "    recordVideo();\n",
        "  \"\"\"\n",
        "\n",
        "  print(\"üé• Grabando... (m√°x\", segundos, \"seg)\")\n",
        "  data_url = output.eval_js(js)\n",
        "  print(\"Finalizado, guardando archivo...\")\n",
        "\n",
        "  # Extraer solo el Base64\n",
        "  base64_data = data_url.split(\",\")[1]\n",
        "\n",
        "  # ARREGLAR PADDING DE BASE64 MANUALMENTE (la causa de tu error)\n",
        "  missing_padding = len(base64_data) % 4\n",
        "  if missing_padding != 0:\n",
        "      base64_data += \"=\" * (4 - missing_padding)\n",
        "\n",
        "  video_bytes = base64.b64decode(base64_data)\n",
        "\n",
        "  with open(VIDEO_PATH, \"wb\") as f:\n",
        "      f.write(video_bytes)\n",
        "\n",
        "  print(\"üìÅ Archivo guardado como:\", VIDEO_PATH)\n",
        "\n",
        "# Ejecutar\n",
        "grabar_video(10)\n",
        "\n",
        "clip = VideoFileClip(VIDEO_PATH)\n",
        "print(\"Duraci√≥n del video:\", clip.duration, \"segundos\")\n",
        "\n",
        "# Guardar audio a WAV\n",
        "AUDIO_PATH = \"audio.wav\"\n",
        "clip.audio.write_audiofile(AUDIO_PATH)\n",
        "print(\"Audio guardado en\", AUDIO_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "5EUDv0bH3bPV",
        "outputId": "bf15eeb2-eed3-46f7-d92e-024ae6b20e03"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üé• Grabando... (m√°x 10 seg)\n",
            "Finalizado, guardando archivo...\n",
            "üìÅ Archivo guardado como: captura.webm\n",
            "Duraci√≥n del video: 9.93 segundos\n",
            "MoviePy - Writing audio in audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Audio guardado en audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probar ambos modelos"
      ],
      "metadata": {
        "id": "iPOLuzoWpQgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_labels = text_le.classes_\n",
        "\n",
        "# 1) Probabilidades desde el video (keypoints)\n",
        "p_video = predict_emotion_from_video(clip, img_model, fps_sample=5)\n",
        "print(\"Probabilidades desde VIDEO (keypoints):\")\n",
        "if p_video is not None:\n",
        "    for label, p in zip(emotion_labels, p_video):\n",
        "        print(f\"  {label}: {p*100:.1f}%\")\n",
        "else:\n",
        "    print(\"  No se pudo estimar (no se detect√≥ rostro).\")\n",
        "\n",
        "# 2) Probabilidades desde el texto (audio transcrito)\n",
        "texto = transcribe_audio(AUDIO_PATH, language=\"es-MX\")\n",
        "print(\"\\nTranscripci√≥n de audio:\")\n",
        "print(texto)\n",
        "\n",
        "p_text = predict_emotion_from_text(texto, text_model, emb_model)\n",
        "print(\"\\nProbabilidades desde TEXTO:\")\n",
        "if p_text is not None:\n",
        "    for label, p in zip(emotion_labels, p_text):\n",
        "        print(f\"  {label}: {p*100:.1f}%\")\n",
        "else:\n",
        "    print(\"  No se pudo estimar (texto vac√≠o o error).\")\n",
        "\n",
        "# 3) Fusi√≥n\n",
        "p_final = fuse_probs(p_video, p_text, alpha=0.5)\n",
        "\n",
        "print(\"\\n=== PREDICCI√ìN FINAL FUSIONADA ===\")\n",
        "if p_final is not None:\n",
        "    for label, p in zip(emotion_labels, p_final):\n",
        "        print(f\"  {label}: {p*100:.1f}%\")\n",
        "    best_idx = np.argmax(p_final)\n",
        "    print(\"\\nEmoci√≥n final predicha:\", emotion_labels[best_idx])\n",
        "else:\n",
        "    print(\"No se pudo obtener una predicci√≥n final.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6EklglSjvgW",
        "outputId": "71e92d57-260a-4df5-c8b7-db2cbc461e66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilidades desde VIDEO (keypoints):\n",
            "  angry: 76.7%\n",
            "  happy: 11.2%\n",
            "  neutral: 1.4%\n",
            "  sad: 10.7%\n",
            "\n",
            "Transcripci√≥n de audio:\n",
            "S√≠ la verdad es que estoy muy triste porque me fue muy mal y pues ni modo es lo que hay as√≠ es la vida a veces se gana a veces se pierde no O sea que\n",
            "\n",
            "Probabilidades desde TEXTO:\n",
            "  angry: 0.2%\n",
            "  happy: 0.2%\n",
            "  neutral: 0.3%\n",
            "  sad: 99.3%\n",
            "\n",
            "=== PREDICCI√ìN FINAL FUSIONADA ===\n",
            "  angry: 38.5%\n",
            "  happy: 5.7%\n",
            "  neutral: 0.9%\n",
            "  sad: 55.0%\n",
            "\n",
            "Emoci√≥n final predicha: sad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9CU-nsv4kqHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}